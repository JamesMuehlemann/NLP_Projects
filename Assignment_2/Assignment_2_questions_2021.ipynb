{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1.Describe the class of strings matched by the following regular expressions.No code is needed and just describe what the following regular expressions do/match).\n",
    "\n",
    "#### a.[a-zA-Z]+\n",
    "#### b.[A-Z][a-z]*\n",
    "#### c.p[aeiou]{,2}t\n",
    "#### d.\\d+(\\.\\d+)?\n",
    "#### e.([^aeiou][aeiou][^aeiou])*\n",
    "#### f.\\w+(?:[-']\\w+)*|[-.(]+|\\S\\w*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### a. String has one or more instances of uppercase or lowercase letters\n",
    "#### b. String has one instance of A-Z(uppercase) and then zero or more instances of a-z(lowercase)\n",
    "#### c. String starts with the letter p, followed by two vowels, and ends with t\n",
    "#### d. String contains one or more digits followed by a . and then followed by one or more digits but the numbers after\n",
    "####    the . can happen once or not at all\n",
    "#### e. String contains zero or more instances of not starting with a vowel, then containing a vowel, and then \n",
    "####    not ending with a vowel \n",
    "#### f. String contains one or more word character and then groups but does not capture any character of ''' \n",
    "####    followed by one or more word characters - this can be done zero or more times or expression includes one or more of \n",
    "####    the -,., or ( OR contains zero or more instances of a character that is not a white space and is a word character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Rewrite the following loop as a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 4), ('is', 2), ('an', 2), ('introduction', 12), ('class', 5)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sent = ['This', 'is', 'an', 'introduction', 'class']\n",
    "#result = []\n",
    "# for word in sent:\n",
    "#    word_len = (word, len(word))\n",
    "#    result.append(word_len)\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 4), ('is', 2), ('an', 2), ('introduction', 12), ('class', 5)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = ['This', 'is', 'an', 'introduction', 'class']\n",
    "result = [(word, len(word)) for word in sent]\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Read in some text from your own document in your local disk, tokenize it, and use the regular expressions to  print the list of all wh-word types that occur. (wh-words in English are used in questions, relative clauses and exclamations: who, which, what, and so on.) (hint: import nltk;  import re; from nltk import word_tokenize) (use lower() and set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = open('sample_text_2_3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What questions often run through my head? What will I eat tonight? When is the sky green? Why does the sun not have a smile like it does on the Raisin Bran box. These are the thoughts in which I am at odds with, and need to understand.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What questions often run through my head? What will I eat tonight? When is the sky green? Why does the sun not have a smile like it does on the Raisin Bran box. These are the thoughts in which I am at odds with, and need to understand.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_doc = open('sample_text_2_3.txt').read()\n",
    "raw_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'What', 'When', 'Why', 'which'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_tokens = word_tokenize(raw_doc)\n",
    "set(t for t in doc_tokens if re.search('^wh', t.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Create your own file consisting of words and (made up) frequencies, where each line consists of a word, the space character, and a positive integer, e.g. fuzzy 53. Read the file into a Python list using open(filename).readlines(). Next, break each line into its two fields using split(), and convert the number into an integer using int(). The result should be a list of the form: [['fuzzy', 53], ...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []\n",
    "text_4 = open('text_2_4.txt').readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in text_4:\n",
    "    line = line.split()\n",
    "    new_list.append([line[0],int(line[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['kermit', 12], ['taboo', 94], ['yahoo', 23], ['wahoo', 76], ['monkeypox', 9]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 .Readability measures are used to score the reading difficulty of a text, for the purposes of selecting texts of appropriate difficulty for language learners. Let us define μw to be the average number of letters per word, and μs to be the average number of words per sentence, in a given text. The Automated Readability Index (ARI) of the text is defined to be: 4.71 μw + 0.5 μs - 21.43. Compute the ARI score for each section of the Brown Corpus (i.e. News, Editorial,…, Humor). Please use two ways introduced in the class to calculate the average number of letters per word μw and the average number of words per sentences μs. You are supposed to get two different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adventure 24.40926324686098\n",
      "belles_lettres 31.095382353662615\n",
      "editorial 30.03411587902211\n",
      "fiction 25.150998441603406\n",
      "government 32.8804612028414\n",
      "hobbies 29.139922001742477\n",
      "humor 28.06731665647837\n",
      "learned 31.912251710365197\n",
      "lore 30.387050143555797\n",
      "mystery 24.159818927038003\n",
      "news 30.83395561360988\n",
      "religion 30.382863961972696\n",
      "reviews 31.13075310191683\n",
      "romance 24.713170386382934\n",
      "science_fiction 25.292578032827997\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "for category in nltk.corpus.brown.categories():\n",
    "    num_chars = len(nltk.corpus.brown.raw(categories = category))\n",
    "    num_words = len(nltk.corpus.brown.words(categories = category))\n",
    "    num_sents = len(nltk.corpus.brown.sents(categories = category))\n",
    "    mu_w = (num_chars) / (num_words)\n",
    "    mu_s = (num_words) / (num_sents)\n",
    "    print(category, ((4.71 * mu_w) + (.5 * mu_s) - 21.43))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adventure 4.0841684990890705\n",
      "belles_lettres 10.987652885621749\n",
      "editorial 9.471025332953673\n",
      "fiction 4.9104735321302115\n",
      "government 12.08430349501021\n",
      "hobbies 8.922356393630267\n",
      "humor 7.887805248319808\n",
      "learned 11.926007043317348\n",
      "lore 10.254756197101155\n",
      "mystery 3.8335518942055167\n",
      "news 10.176684595052684\n",
      "religion 10.203109907301261\n",
      "reviews 10.769699888473433\n",
      "romance 4.34922419804213\n",
      "science_fiction 4.978058336905399\n"
     ]
    }
   ],
   "source": [
    "for category in nltk.corpus.brown.categories():\n",
    "    words = nltk.corpus.brown.words(categories = category)\n",
    "    sents = nltk.corpus.brown.sents(categories = category)\n",
    "    mu_w = sum(len(w) for w in words) / len(words)\n",
    "    mu_s = sum(len(s) for s in sents) / len(sents)\n",
    "    print(category, ((4.71 * mu_w) + (.5 * mu_s) - 21.43))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I tried to use the slides but not sure if these produced the calculations you were hoping. Hope to go over this question in class as it took me a long time to finally get. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.Use the Porter Stemmer to normalize some tokenized text (see below), calling the stemmer on each word. Do the same thing with the Lancaster Stemmer and describe any difference you observe by using these two stemmers. Finally, please try Lemmatization by using WordNet Lemmatizer and describe any difference you observe compared to Porter Stemmer and Lancaster Stemmer.\n",
    "\n",
    "#### text='Technologies based on NLP are becoming increasingly widespread. For example, phones and handheld computers support predictive text and handwriting recognition; web search engines give access to information locked up in unstructured text; machine translation allows us to retrieve texts written in Chinese and read them in Spanish; text analysis enables us to detect sentiment in tweets and blogs. By providing more natural human-machine interfaces, and more sophisticated access to stored information, language processing has come to play a central role in the multilingual information society'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_6 ='Technologies based on NLP are becoming increasingly widespread. For example, phones and handheld computers support predictive text and handwriting recognition; web search engines give access to information locked up in unstructured text; machine translation allows us to retrieve texts written in Chinese and read them in Spanish; text analysis enables us to detect sentiment in tweets and blogs. By providing more natural human-machine interfaces, and more sophisticated access to stored information, language processing has come to play a central role in the multilingual information society'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_6 = word_tokenize(text_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['technolog',\n",
       " 'base',\n",
       " 'on',\n",
       " 'nlp',\n",
       " 'are',\n",
       " 'becom',\n",
       " 'increasingli',\n",
       " 'widespread',\n",
       " '.',\n",
       " 'for',\n",
       " 'exampl',\n",
       " ',',\n",
       " 'phone',\n",
       " 'and',\n",
       " 'handheld',\n",
       " 'comput',\n",
       " 'support',\n",
       " 'predict',\n",
       " 'text',\n",
       " 'and',\n",
       " 'handwrit',\n",
       " 'recognit',\n",
       " ';',\n",
       " 'web',\n",
       " 'search',\n",
       " 'engin',\n",
       " 'give',\n",
       " 'access',\n",
       " 'to',\n",
       " 'inform',\n",
       " 'lock',\n",
       " 'up',\n",
       " 'in',\n",
       " 'unstructur',\n",
       " 'text',\n",
       " ';',\n",
       " 'machin',\n",
       " 'translat',\n",
       " 'allow',\n",
       " 'us',\n",
       " 'to',\n",
       " 'retriev',\n",
       " 'text',\n",
       " 'written',\n",
       " 'in',\n",
       " 'chines',\n",
       " 'and',\n",
       " 'read',\n",
       " 'them',\n",
       " 'in',\n",
       " 'spanish',\n",
       " ';',\n",
       " 'text',\n",
       " 'analysi',\n",
       " 'enabl',\n",
       " 'us',\n",
       " 'to',\n",
       " 'detect',\n",
       " 'sentiment',\n",
       " 'in',\n",
       " 'tweet',\n",
       " 'and',\n",
       " 'blog',\n",
       " '.',\n",
       " 'By',\n",
       " 'provid',\n",
       " 'more',\n",
       " 'natur',\n",
       " 'human-machin',\n",
       " 'interfac',\n",
       " ',',\n",
       " 'and',\n",
       " 'more',\n",
       " 'sophist',\n",
       " 'access',\n",
       " 'to',\n",
       " 'store',\n",
       " 'inform',\n",
       " ',',\n",
       " 'languag',\n",
       " 'process',\n",
       " 'ha',\n",
       " 'come',\n",
       " 'to',\n",
       " 'play',\n",
       " 'a',\n",
       " 'central',\n",
       " 'role',\n",
       " 'in',\n",
       " 'the',\n",
       " 'multilingu',\n",
       " 'inform',\n",
       " 'societi']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[porter.stem(t) for t in tokens_6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['technolog',\n",
       " 'bas',\n",
       " 'on',\n",
       " 'nlp',\n",
       " 'ar',\n",
       " 'becom',\n",
       " 'increas',\n",
       " 'widespread',\n",
       " '.',\n",
       " 'for',\n",
       " 'exampl',\n",
       " ',',\n",
       " 'phon',\n",
       " 'and',\n",
       " 'handheld',\n",
       " 'comput',\n",
       " 'support',\n",
       " 'predict',\n",
       " 'text',\n",
       " 'and',\n",
       " 'handwrit',\n",
       " 'recognit',\n",
       " ';',\n",
       " 'web',\n",
       " 'search',\n",
       " 'engin',\n",
       " 'giv',\n",
       " 'access',\n",
       " 'to',\n",
       " 'inform',\n",
       " 'lock',\n",
       " 'up',\n",
       " 'in',\n",
       " 'unstruct',\n",
       " 'text',\n",
       " ';',\n",
       " 'machin',\n",
       " 'transl',\n",
       " 'allow',\n",
       " 'us',\n",
       " 'to',\n",
       " 'retriev',\n",
       " 'text',\n",
       " 'writ',\n",
       " 'in',\n",
       " 'chines',\n",
       " 'and',\n",
       " 'read',\n",
       " 'them',\n",
       " 'in',\n",
       " 'span',\n",
       " ';',\n",
       " 'text',\n",
       " 'analys',\n",
       " 'en',\n",
       " 'us',\n",
       " 'to',\n",
       " 'detect',\n",
       " 'senty',\n",
       " 'in',\n",
       " 'tweet',\n",
       " 'and',\n",
       " 'blog',\n",
       " '.',\n",
       " 'by',\n",
       " 'provid',\n",
       " 'mor',\n",
       " 'nat',\n",
       " 'human-machine',\n",
       " 'interfac',\n",
       " ',',\n",
       " 'and',\n",
       " 'mor',\n",
       " 'soph',\n",
       " 'access',\n",
       " 'to',\n",
       " 'stor',\n",
       " 'inform',\n",
       " ',',\n",
       " 'langu',\n",
       " 'process',\n",
       " 'has',\n",
       " 'com',\n",
       " 'to',\n",
       " 'play',\n",
       " 'a',\n",
       " 'cent',\n",
       " 'rol',\n",
       " 'in',\n",
       " 'the',\n",
       " 'multil',\n",
       " 'inform',\n",
       " 'socy']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lancaster.stem(t) for t in tokens_6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER:\n",
    "#### Porter seems like a much less aggressive stemmer because it cuts off words much later on than with lancaster that cuts off words much earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Technologies',\n",
       " 'based',\n",
       " 'on',\n",
       " 'NLP',\n",
       " 'are',\n",
       " 'becoming',\n",
       " 'increasingly',\n",
       " 'widespread',\n",
       " '.',\n",
       " 'For',\n",
       " 'example',\n",
       " ',',\n",
       " 'phone',\n",
       " 'and',\n",
       " 'handheld',\n",
       " 'computer',\n",
       " 'support',\n",
       " 'predictive',\n",
       " 'text',\n",
       " 'and',\n",
       " 'handwriting',\n",
       " 'recognition',\n",
       " ';',\n",
       " 'web',\n",
       " 'search',\n",
       " 'engine',\n",
       " 'give',\n",
       " 'access',\n",
       " 'to',\n",
       " 'information',\n",
       " 'locked',\n",
       " 'up',\n",
       " 'in',\n",
       " 'unstructured',\n",
       " 'text',\n",
       " ';',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'allows',\n",
       " 'u',\n",
       " 'to',\n",
       " 'retrieve',\n",
       " 'text',\n",
       " 'written',\n",
       " 'in',\n",
       " 'Chinese',\n",
       " 'and',\n",
       " 'read',\n",
       " 'them',\n",
       " 'in',\n",
       " 'Spanish',\n",
       " ';',\n",
       " 'text',\n",
       " 'analysis',\n",
       " 'enables',\n",
       " 'u',\n",
       " 'to',\n",
       " 'detect',\n",
       " 'sentiment',\n",
       " 'in',\n",
       " 'tweet',\n",
       " 'and',\n",
       " 'blog',\n",
       " '.',\n",
       " 'By',\n",
       " 'providing',\n",
       " 'more',\n",
       " 'natural',\n",
       " 'human-machine',\n",
       " 'interface',\n",
       " ',',\n",
       " 'and',\n",
       " 'more',\n",
       " 'sophisticated',\n",
       " 'access',\n",
       " 'to',\n",
       " 'stored',\n",
       " 'information',\n",
       " ',',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'ha',\n",
       " 'come',\n",
       " 'to',\n",
       " 'play',\n",
       " 'a',\n",
       " 'central',\n",
       " 'role',\n",
       " 'in',\n",
       " 'the',\n",
       " 'multilingual',\n",
       " 'information',\n",
       " 'society']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[wnl.lemmatize(t) for t in tokens_6]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER:\n",
    "#### This lemmatizer seems much less aggressive than the other 2 because it keeps words only if they are still a word in the dictionary after they have been reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Please try to retreive some text from any web page in the form of HTML documents, tokenize the text, create an NLTK text and then use similar( ) function on any word of your interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waterfront tone acting godfather gun roof front way\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "url='https://www.rogerebert.com/reviews/great-movie-on-the-waterfront-1954'\n",
    "html = request.urlopen(url).read().decode('utf8')\n",
    "raw_7 = BeautifulSoup(html, 'html.parser').get_text()\n",
    "tokens_7 = word_tokenize(raw_7)\n",
    "text_7 = nltk.Text(tokens_7)\n",
    "text_7.similar('film')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.Rewrite the following nested loop by using a list comprehension and regular expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  words = ['attribution', 'confabulation', 'elocution',\n",
    "         'sequoia', 'tenacious', 'unidirectional']\n",
    "  vsequences = set()\n",
    "  for word in words:\n",
    "      vowels = []\n",
    "      for char in word:\n",
    "          if char in 'aeiou':\n",
    "              vowels.append(char)\n",
    "      vsequences.add(''.join(vowels))\n",
    "  sorted(vsequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['attribution', 'confabulation', 'elocution', 'sequoia', 'tenacious', 'unidirectional']\n",
    "sorted([''.join([char for char in word if re.search('[aeiou]',char)]) for word in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.Try to refer the following sample code to print the following sentences in a formatted way.(Hint: you should use str.format() method in print() and for loop；For more information, please read the textbook section 3.9 in chapter 3) \n",
    "#### Output should exactly look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lee wants a sandwich right now\n",
      "Lee wants a spam fritter right now\n",
      "Lee wants a pancake right now\n"
     ]
    }
   ],
   "source": [
    "The Tragedie of Hamlet was written by William Shakespeare in 1599\n",
    "Leaves of Grass        was written by Walt Whiteman       in 1855\n",
    "Emma                   was written by Jane Austen         in 1816\n",
    "# sample code:\n",
    "template = 'Lee wants a {} right now'\n",
    "menu = ['sandwich', 'spam fritter', 'pancake']\n",
    "for snack in menu:\n",
    "    print(template.format(snack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Tragedie of Hamlet was written by William Shakespeare in 1599\n",
      "Leaves of Grass        was written by Walt Whitman        in 1855\n",
      "Emma                   was written by Jane Austen         in 1816\n"
     ]
    }
   ],
   "source": [
    "title = ['The Tragedie of Hamlet', 'Leaves of Grass','Emma']\n",
    "author = ['William Shakespeare','Walt Whitman','Jane Austen']\n",
    "year = ['1599', '1855', '1816']\n",
    "template_9 = '{:22s} was written by {:19s} in {}'\n",
    "for i in range (len(title)):\n",
    "    print(template_9.format(title[i],author[i],year[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Please first use sentence tokenization to print out the first 10 sentences in the \"carroll-alice.txt\" in the Gutenberg corpus. And then use basic corpus functionality sents() to return the first 10 sentences in this book. Please describe the difference between the two results.  (hint: use sent_tokenzie (), pprint(), sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\",\n",
      " 'Down the Rabbit-Hole\\n'\n",
      " '\\n'\n",
      " 'Alice was beginning to get very tired of sitting by her sister on the\\n'\n",
      " 'bank, and of having nothing to do: once or twice she had peeped into the\\n'\n",
      " 'book her sister was reading, but it had no pictures or conversations in\\n'\n",
      " \"it, 'and what is the use of a book,' thought Alice 'without pictures or\\n\"\n",
      " \"conversation?'\",\n",
      " 'So she was considering in her own mind (as well as she could, for the\\n'\n",
      " 'hot day made her feel very sleepy and stupid), whether the pleasure\\n'\n",
      " 'of making a daisy-chain would be worth the trouble of getting up and\\n'\n",
      " 'picking the daisies, when suddenly a White Rabbit with pink eyes ran\\n'\n",
      " 'close by her.',\n",
      " 'There was nothing so VERY remarkable in that; nor did Alice think it so\\n'\n",
      " \"VERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\",\n",
      " 'Oh dear!',\n",
      " \"I shall be late!'\",\n",
      " '(when she thought it over afterwards, it\\n'\n",
      " 'occurred to her that she ought to have wondered at this, but at the time\\n'\n",
      " 'it all seemed quite natural); but when the Rabbit actually TOOK A WATCH\\n'\n",
      " 'OUT OF ITS WAISTCOAT-POCKET, and looked at it, and then hurried on,\\n'\n",
      " 'Alice started to her feet, for it flashed across her mind that she had\\n'\n",
      " 'never before seen a rabbit with either a waistcoat-pocket, or a watch\\n'\n",
      " 'to take out of it, and burning with curiosity, she ran across the field\\n'\n",
      " 'after it, and fortunately was just in time to see it pop down a large\\n'\n",
      " 'rabbit-hole under the hedge.',\n",
      " 'In another moment down went Alice after it, never once considering how\\n'\n",
      " 'in the world she was to get out again.',\n",
      " 'The rabbit-hole went straight on like a tunnel for some way, and then\\n'\n",
      " 'dipped suddenly down, so suddenly that Alice had not a moment to think\\n'\n",
      " 'about stopping herself before she found herself falling down a very deep\\n'\n",
      " 'well.',\n",
      " 'Either the well was very deep, or she fell very slowly, for she had\\n'\n",
      " 'plenty of time as she went down to look about her and to wonder what was\\n'\n",
      " 'going to happen next.']\n"
     ]
    }
   ],
   "source": [
    "alice_text = nltk.corpus.gutenberg.raw('carroll-alice.txt')\n",
    "alice_sents = sent_tokenize(alice_text)\n",
    "pprint.pprint(alice_sents[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[',\n",
       "  'Alice',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'Adventures',\n",
       "  'in',\n",
       "  'Wonderland',\n",
       "  'by',\n",
       "  'Lewis',\n",
       "  'Carroll',\n",
       "  '1865',\n",
       "  ']'],\n",
       " ['CHAPTER', 'I', '.'],\n",
       " ['Down', 'the', 'Rabbit', '-', 'Hole'],\n",
       " ['Alice',\n",
       "  'was',\n",
       "  'beginning',\n",
       "  'to',\n",
       "  'get',\n",
       "  'very',\n",
       "  'tired',\n",
       "  'of',\n",
       "  'sitting',\n",
       "  'by',\n",
       "  'her',\n",
       "  'sister',\n",
       "  'on',\n",
       "  'the',\n",
       "  'bank',\n",
       "  ',',\n",
       "  'and',\n",
       "  'of',\n",
       "  'having',\n",
       "  'nothing',\n",
       "  'to',\n",
       "  'do',\n",
       "  ':',\n",
       "  'once',\n",
       "  'or',\n",
       "  'twice',\n",
       "  'she',\n",
       "  'had',\n",
       "  'peeped',\n",
       "  'into',\n",
       "  'the',\n",
       "  'book',\n",
       "  'her',\n",
       "  'sister',\n",
       "  'was',\n",
       "  'reading',\n",
       "  ',',\n",
       "  'but',\n",
       "  'it',\n",
       "  'had',\n",
       "  'no',\n",
       "  'pictures',\n",
       "  'or',\n",
       "  'conversations',\n",
       "  'in',\n",
       "  'it',\n",
       "  ',',\n",
       "  \"'\",\n",
       "  'and',\n",
       "  'what',\n",
       "  'is',\n",
       "  'the',\n",
       "  'use',\n",
       "  'of',\n",
       "  'a',\n",
       "  'book',\n",
       "  \",'\",\n",
       "  'thought',\n",
       "  'Alice',\n",
       "  \"'\",\n",
       "  'without',\n",
       "  'pictures',\n",
       "  'or',\n",
       "  'conversation',\n",
       "  \"?'\"],\n",
       " ['So',\n",
       "  'she',\n",
       "  'was',\n",
       "  'considering',\n",
       "  'in',\n",
       "  'her',\n",
       "  'own',\n",
       "  'mind',\n",
       "  '(',\n",
       "  'as',\n",
       "  'well',\n",
       "  'as',\n",
       "  'she',\n",
       "  'could',\n",
       "  ',',\n",
       "  'for',\n",
       "  'the',\n",
       "  'hot',\n",
       "  'day',\n",
       "  'made',\n",
       "  'her',\n",
       "  'feel',\n",
       "  'very',\n",
       "  'sleepy',\n",
       "  'and',\n",
       "  'stupid',\n",
       "  '),',\n",
       "  'whether',\n",
       "  'the',\n",
       "  'pleasure',\n",
       "  'of',\n",
       "  'making',\n",
       "  'a',\n",
       "  'daisy',\n",
       "  '-',\n",
       "  'chain',\n",
       "  'would',\n",
       "  'be',\n",
       "  'worth',\n",
       "  'the',\n",
       "  'trouble',\n",
       "  'of',\n",
       "  'getting',\n",
       "  'up',\n",
       "  'and',\n",
       "  'picking',\n",
       "  'the',\n",
       "  'daisies',\n",
       "  ',',\n",
       "  'when',\n",
       "  'suddenly',\n",
       "  'a',\n",
       "  'White',\n",
       "  'Rabbit',\n",
       "  'with',\n",
       "  'pink',\n",
       "  'eyes',\n",
       "  'ran',\n",
       "  'close',\n",
       "  'by',\n",
       "  'her',\n",
       "  '.'],\n",
       " ['There',\n",
       "  'was',\n",
       "  'nothing',\n",
       "  'so',\n",
       "  'VERY',\n",
       "  'remarkable',\n",
       "  'in',\n",
       "  'that',\n",
       "  ';',\n",
       "  'nor',\n",
       "  'did',\n",
       "  'Alice',\n",
       "  'think',\n",
       "  'it',\n",
       "  'so',\n",
       "  'VERY',\n",
       "  'much',\n",
       "  'out',\n",
       "  'of',\n",
       "  'the',\n",
       "  'way',\n",
       "  'to',\n",
       "  'hear',\n",
       "  'the',\n",
       "  'Rabbit',\n",
       "  'say',\n",
       "  'to',\n",
       "  'itself',\n",
       "  ',',\n",
       "  \"'\",\n",
       "  'Oh',\n",
       "  'dear',\n",
       "  '!'],\n",
       " ['Oh', 'dear', '!'],\n",
       " ['I', 'shall', 'be', 'late', \"!'\"],\n",
       " ['(',\n",
       "  'when',\n",
       "  'she',\n",
       "  'thought',\n",
       "  'it',\n",
       "  'over',\n",
       "  'afterwards',\n",
       "  ',',\n",
       "  'it',\n",
       "  'occurred',\n",
       "  'to',\n",
       "  'her',\n",
       "  'that',\n",
       "  'she',\n",
       "  'ought',\n",
       "  'to',\n",
       "  'have',\n",
       "  'wondered',\n",
       "  'at',\n",
       "  'this',\n",
       "  ',',\n",
       "  'but',\n",
       "  'at',\n",
       "  'the',\n",
       "  'time',\n",
       "  'it',\n",
       "  'all',\n",
       "  'seemed',\n",
       "  'quite',\n",
       "  'natural',\n",
       "  ');',\n",
       "  'but',\n",
       "  'when',\n",
       "  'the',\n",
       "  'Rabbit',\n",
       "  'actually',\n",
       "  'TOOK',\n",
       "  'A',\n",
       "  'WATCH',\n",
       "  'OUT',\n",
       "  'OF',\n",
       "  'ITS',\n",
       "  'WAISTCOAT',\n",
       "  '-',\n",
       "  'POCKET',\n",
       "  ',',\n",
       "  'and',\n",
       "  'looked',\n",
       "  'at',\n",
       "  'it',\n",
       "  ',',\n",
       "  'and',\n",
       "  'then',\n",
       "  'hurried',\n",
       "  'on',\n",
       "  ',',\n",
       "  'Alice',\n",
       "  'started',\n",
       "  'to',\n",
       "  'her',\n",
       "  'feet',\n",
       "  ',',\n",
       "  'for',\n",
       "  'it',\n",
       "  'flashed',\n",
       "  'across',\n",
       "  'her',\n",
       "  'mind',\n",
       "  'that',\n",
       "  'she',\n",
       "  'had',\n",
       "  'never',\n",
       "  'before',\n",
       "  'seen',\n",
       "  'a',\n",
       "  'rabbit',\n",
       "  'with',\n",
       "  'either',\n",
       "  'a',\n",
       "  'waistcoat',\n",
       "  '-',\n",
       "  'pocket',\n",
       "  ',',\n",
       "  'or',\n",
       "  'a',\n",
       "  'watch',\n",
       "  'to',\n",
       "  'take',\n",
       "  'out',\n",
       "  'of',\n",
       "  'it',\n",
       "  ',',\n",
       "  'and',\n",
       "  'burning',\n",
       "  'with',\n",
       "  'curiosity',\n",
       "  ',',\n",
       "  'she',\n",
       "  'ran',\n",
       "  'across',\n",
       "  'the',\n",
       "  'field',\n",
       "  'after',\n",
       "  'it',\n",
       "  ',',\n",
       "  'and',\n",
       "  'fortunately',\n",
       "  'was',\n",
       "  'just',\n",
       "  'in',\n",
       "  'time',\n",
       "  'to',\n",
       "  'see',\n",
       "  'it',\n",
       "  'pop',\n",
       "  'down',\n",
       "  'a',\n",
       "  'large',\n",
       "  'rabbit',\n",
       "  '-',\n",
       "  'hole',\n",
       "  'under',\n",
       "  'the',\n",
       "  'hedge',\n",
       "  '.'],\n",
       " ['In',\n",
       "  'another',\n",
       "  'moment',\n",
       "  'down',\n",
       "  'went',\n",
       "  'Alice',\n",
       "  'after',\n",
       "  'it',\n",
       "  ',',\n",
       "  'never',\n",
       "  'once',\n",
       "  'considering',\n",
       "  'how',\n",
       "  'in',\n",
       "  'the',\n",
       "  'world',\n",
       "  'she',\n",
       "  'was',\n",
       "  'to',\n",
       "  'get',\n",
       "  'out',\n",
       "  'again',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'rabbit',\n",
       "  '-',\n",
       "  'hole',\n",
       "  'went',\n",
       "  'straight',\n",
       "  'on',\n",
       "  'like',\n",
       "  'a',\n",
       "  'tunnel',\n",
       "  'for',\n",
       "  'some',\n",
       "  'way',\n",
       "  ',',\n",
       "  'and',\n",
       "  'then',\n",
       "  'dipped',\n",
       "  'suddenly',\n",
       "  'down',\n",
       "  ',',\n",
       "  'so',\n",
       "  'suddenly',\n",
       "  'that',\n",
       "  'Alice',\n",
       "  'had',\n",
       "  'not',\n",
       "  'a',\n",
       "  'moment',\n",
       "  'to',\n",
       "  'think',\n",
       "  'about',\n",
       "  'stopping',\n",
       "  'herself',\n",
       "  'before',\n",
       "  'she',\n",
       "  'found',\n",
       "  'herself',\n",
       "  'falling',\n",
       "  'down',\n",
       "  'a',\n",
       "  'very',\n",
       "  'deep',\n",
       "  'well',\n",
       "  '.'],\n",
       " ['Either',\n",
       "  'the',\n",
       "  'well',\n",
       "  'was',\n",
       "  'very',\n",
       "  'deep',\n",
       "  ',',\n",
       "  'or',\n",
       "  'she',\n",
       "  'fell',\n",
       "  'very',\n",
       "  'slowly',\n",
       "  ',',\n",
       "  'for',\n",
       "  'she',\n",
       "  'had',\n",
       "  'plenty',\n",
       "  'of',\n",
       "  'time',\n",
       "  'as',\n",
       "  'she',\n",
       "  'went',\n",
       "  'down',\n",
       "  'to',\n",
       "  'look',\n",
       "  'about',\n",
       "  'her',\n",
       "  'and',\n",
       "  'to',\n",
       "  'wonder',\n",
       "  'what',\n",
       "  'was',\n",
       "  'going',\n",
       "  'to',\n",
       "  'happen',\n",
       "  'next',\n",
       "  '.']]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice = nltk.corpus.gutenberg.sents('carroll-alice.txt')\n",
    "alice[:12]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER:\n",
    "#### The difference in these two ways of retrieving the first 10 sentences is that the second way creates a list of sentences that each get printed on a separate line. This also does not show the new line character. In the first way we are using sentence segmentation to tokenize each sentence of the text.  The sent_tokenize function also splits sentences in quotes into individual strings. With the sent() function we can get more granular with the data, and delve into each sentence of the first 10 in the book. We do not have that same liberty when using the sent_tokenize() function as it only creates one list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
