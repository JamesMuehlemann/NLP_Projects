{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Using Naive Bayes classifier described in this chapter, and any features you can think of, build the best name gender classifier you can. Begin by splitting the Names Corpus into three subsets: 500 words for the test set, 500 words for the dev-test set, and the remaining 6900 words for the training set. Then, starting with the example name gender classifier, make incremental improvements. Use the dev-test set to check your progress. Once you are satisfied with your classifier, check its final performance on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    return {'suffix1': word[-1:], 'suffix2': word[-2:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features2(name):\n",
    "    features = {}\n",
    "    features[\"first_letter\"] = name[0].lower()\n",
    "    features[\"last_letter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count({})\".format(letter)] = name.lower().count(letter)\n",
    "        features[\"has({})\".format(letter)] = (letter in name.lower())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] +\n",
    "                 [(name, 'female') for name in names.words('female.txt')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_names = labeled_names[:500]\n",
    "devtest_names = labeled_names[500:1000]\n",
    "train_names = labeled_names[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.696\n"
     ]
    }
   ],
   "source": [
    "train_set = [(gender_features(n), gender) for (n, gender) in train_names]\n",
    "devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]\n",
    "test_set = [(gender_features(n), gender) for (n, gender) in test_names]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, devtest_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Using the movie review document classifier discussed in Chapter 6- Section 1.3 ( constructing a list of the 2500 most frequent words as features and use the first 150 documents as the test dataset) , generate a list of the 10 features that the classifier finds to be most informative. Can you explain why these particular features are informative? Do you find any of them surprising?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:2500]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[150:], featuresets[:150]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8266666666666667\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "   contains(outstanding) = True              pos : neg    =     11.3 : 1.0\n",
      "        contains(finest) = True              pos : neg    =      8.0 : 1.0\n",
      "         contains(damon) = True              pos : neg    =      7.9 : 1.0\n",
      "         contains(mulan) = True              pos : neg    =      7.7 : 1.0\n",
      "        contains(seagal) = True              neg : pos    =      7.3 : 1.0\n",
      "   contains(wonderfully) = True              pos : neg    =      7.3 : 1.0\n",
      "    contains(schumacher) = True              neg : pos    =      6.9 : 1.0\n",
      "          contains(lame) = True              neg : pos    =      5.9 : 1.0\n",
      "         contains(flynt) = True              pos : neg    =      5.7 : 1.0\n",
      "      contains(lebowski) = True              pos : neg    =      5.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These make sense as some include last names of a-list actors and positive adjectives. One that does surprise me is lame as that is usually not used to describe a positive review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Select one of the classification tasks described in this chapter, such as name gender detection, document classification, part-of-speech tagging, or dialog act classification. Using the same training and test data, and the same feature extractor, build three classifiers for the task: a decision tree, a naive Bayes classifier, and a  Maximum Entropy classifier. Compare the performance of the three classifiers on your selected task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = nltk.corpus.treebank_raw.sents()\n",
    "tokens = []\n",
    "boundaries = set()\n",
    "offset = 0\n",
    "for sent in sents:\n",
    "    tokens.extend(sent)\n",
    "    offset += len(sent)\n",
    "    boundaries.add(offset-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_features(tokens, i):\n",
    "    return {'next-word-capitalized': tokens[i+1][0].isupper(),\n",
    "            'prev-word': tokens[i-1].lower(),\n",
    "             'punct': tokens[i],\n",
    "             'prev-word-is-one-char': len(tokens[i-1]) == 1}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(punct_features(tokens, i), (i in boundaries))\n",
    "               for i in range(1, len(tokens)-1)\n",
    "               if tokens[i] in '.?!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(len(featuresets) * .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set,test_set = featuresets[size:], featuresets[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbc_classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.936026936026936"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(nbc_classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.610\n",
      "             2          -0.42328        0.891\n",
      "             3          -0.31744        0.944\n",
      "             4          -0.26043        0.944\n",
      "             5          -0.22517        0.943\n",
      "             6          -0.20121        0.945\n",
      "             7          -0.18373        0.945\n",
      "             8          -0.17027        0.945\n",
      "             9          -0.15951        0.943\n",
      "            10          -0.15063        0.943\n",
      "            11          -0.14314        0.970\n",
      "            12          -0.13671        0.972\n",
      "            13          -0.13112        0.972\n",
      "            14          -0.12619        0.973\n",
      "            15          -0.12182        0.973\n",
      "            16          -0.11790        0.973\n",
      "            17          -0.11436        0.973\n",
      "            18          -0.11116        0.973\n",
      "            19          -0.10824        0.973\n",
      "            20          -0.10556        0.973\n",
      "            21          -0.10310        0.973\n",
      "            22          -0.10083        0.973\n",
      "            23          -0.09873        0.973\n",
      "            24          -0.09678        0.973\n",
      "            25          -0.09496        0.973\n",
      "      Training stopped: keyboard interrupt\n",
      "         Final          -0.09326        0.973\n"
     ]
    }
   ],
   "source": [
    "me_classifier = nltk.MaxentClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_classifier = nltk.DecisionTreeClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9730639730639731"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(dc_classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Identify the NPS Chat Corpus, which was demonstrated in Chapter 2, consists of posts from instant messaging sessions. These posts have all been labeled with one of 15 dialogue act types, such as \"Statement,\" \"Emotion,\" \"ynQuestion\", and \"Continuer.\" We can therefore use this data to build a classifier that can identify the dialogue act types for new instant messaging posts. Build a simple feature extractor that checks what words the post contains. Construct the training and testing data by applying the feature extractor to each post and create a Naïve Bayes classifier. Please print the accuracy of this classifier. Please use NPS Chat Corpus as our dataset and use 8% data as our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = nltk.corpus.nps_chat.xml_posts()[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dialogue_act_features(post):\n",
    "    features = {}\n",
    "    for word in nltk.word_tokenize(post):\n",
    "        features['contains({})'.format(word.lower())] = True\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(dialogue_act_features(post.text), post.get('class'))\n",
    "                for post in posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(len(featuresets) * 0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = featuresets[size:], featuresets[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68125\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Given the following confusion matrix, please calculate: a) Accuracy Rate; b) Precision; c) Recall; d) F-Measure.\n",
    "\n",
    "\tNo\tYes\n",
    "No\t104\t33\n",
    "Yes\t13\t50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos, false_pos, true_neg, false_neg, = 50, 13, 104, 33\n",
    "num_obs = true_pos + false_pos + true_neg + false_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_rate = (true_pos +true_neg) / num_obs\n",
    "precision = true_pos/ (true_pos + false_pos)\n",
    "recall = true_pos / (true_pos + false_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Rate: 0.77\n",
      "Precision: 0.7936507936507936\n",
      "Recall: 0.6024096385542169\n"
     ]
    }
   ],
   "source": [
    " print('Accuracy Rate: %s\\nPrecision: %s\\nRecall: %s' %(accuracy_rate, precision, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Write a tag pattern to match noun phrases containing plural head nouns in the following sentence: \"Many researchers discussed this project for two weeks.\" Try to do this by generalizing the tag pattern that handled singular noun phrases too. Please 1) pos-tag this sentence 2) write a tag pattern (i.e. grammar); 3) use RegexpParser to parse the sentence and 4) print out the result containing NP (noun phrases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Many/JJ researchers/NNS)\n",
      "  discussed/VBD\n",
      "  this/DT\n",
      "  project/NN\n",
      "  for/IN\n",
      "  (NP two/CD weeks/NNS)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "grammar = \"NP:{<DT>?<CD>?<JJ>*<NNS>}\"\n",
    "cp=nltk.RegexpParser(grammar)\n",
    "document = \"Many researchers discussed this project for two weeks.\"\n",
    "sentences = nltk.sent_tokenize(document)\n",
    "sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "sentence = [('Many', 'JJ'), ('researchers', 'NNS'), ('discussed', 'VBD'), \n",
    "        ('this', 'DT'), ('project', 'NN'), ('for', 'IN'), ('two', 'CD'), ('weeks', 'NNS'), ('.', '.')]\n",
    "result = cp.parse(sentence)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Write a tag pattern to cover noun phrases that contain gerunds, e.g. \"the/DT receiving/VBG end/NN\", \"assistant/NN managing/VBG editor/NN\". Add these patterns to the grammar, one per line. Test your work using some tagged sentences of your own devising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Gerunds the/DT receiving/VBG end/NN)\n",
      "(Gerunds assistant/NN managing/VBG editor/NN)\n"
     ]
    }
   ],
   "source": [
    "grammar = \"Gerunds:{<DT>?<NN>?<VBG><NN>}\"\n",
    "sentence = [(\"the\",\"DT\"), (\"receiving\",\"VBG\"), (\"end\",\"NN\"), (\"assistant\",\"NN\"), (\"managing\",\"VBG\"), (\"editor\",\"NN\")]\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "tree = cp.parse(sentence)\n",
    "for subtree in tree.subtrees():\n",
    "    if subtree.label()=='Gerunds': print (subtree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Use the Brown Corpus and the cascaded chunkers that has patterns for noun phrases, prepositional phrases, verb phrases, and clauses to print out all the verb phrases in the Brown corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "  NP: {<DT|JJ|NN.*>+}      \n",
    "  PP: {<IN><NP>}               \n",
    "  VP: {<VB.*><NP|PP|CLAUSE>+$} \n",
    "  CLAUSE: {<NP><VP>}          \n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(VP Ask/VB-HL (NP jail/NN-HL deputies/NNS-HL))\n",
      "(VP revolving/VBG-HL (NP fund/NN-HL))\n",
      "(VP Issue/VB-HL (NP jury/NN-HL subpoenas/NNS-HL))\n",
      "(VP Nursing/VBG-HL (NP home/NN-HL care/NN-HL))\n",
      "(VP pay/VB-HL (NP doctors/NNS-HL))\n",
      "(VP nursing/VBG-HL (NP homes/NNS))\n",
      "(VP Asks/VBZ-HL (NP research/NN-HL funds/NNS-HL))\n",
      "(VP Regrets/VBZ-HL (NP attack/NN-HL))\n",
      "(VP Decries/VBZ-HL (NP joblessness/NN-HL))\n",
      "(VP Underlying/VBG-HL (NP concern/NN-HL))\n",
      "(VP bar/VB-HL (NP vehicles/NNS-HL))\n",
      "(VP loses/VBZ-HL (NP pace/NN-HL))\n",
      "(VP hits/VBZ-HL (NP homer/NN-HL))\n",
      "(VP attend/VB-HL (NP races/NNS-HL))\n",
      "(VP follows/VBZ-HL (NP ceremonies/NNS-HL))\n",
      "(VP Noted/VBN-HL (NP artist/NN-HL))\n",
      "(VP Cites/VBZ-HL (NP discrepancies/NNS-HL))\n",
      "(VP calls/VBZ-HL (NP police/NNS-HL))\n",
      "(VP held/VBN-HL (NP key/NN-HL))\n",
      "(VP grant/VB-HL (NP bail/NN-HL))\n",
      "(VP Held/VBD-HL (NP candle/NN-HL))\n",
      "(VP Expresses/VBZ-HL (NP thanks/NNS-HL))\n",
      "(VP Gets/VBZ-HL (NP car/NN-HL number/NN-HL))\n",
      "(VP Attacks/VBZ-HL (NP officer/NN-HL))\n",
      "(VP oks/VBZ-HL (NP pact/NN-HL))\n",
      "(VP report/VB-HL (NP gains/NNS-HL))\n",
      "(VP Pulling/VBG-HL (NP strings/NNS-HL))\n",
      "(VP United/VBN-TL-HL (NP States/NNS-TL-HL defense/NN-HL))\n",
      "(VP Betting/VBG-HL (NP men/NNS-HL))\n",
      "(VP brings/VBZ-HL (NP numbness/NN-HL))\n",
      "(VP Questions/VBZ-HL (NP shelters/NNS-HL))\n",
      "(VP Marketing/VBG-HL (NP meat/NN-HL))\n",
      "(VP Taxing/VBG-HL (NP improvements/NNS-HL))\n",
      "(VP Praises/VBZ-HL (NP exhibit/NN-HL))\n",
      "(VP aid/VB (NP international/JJ law/NN))\n",
      "(VP retarded/VBN-HL (NP children/NNS-HL))\n",
      "(VP tormented/VBN-HL (NP span/NN-HL))\n",
      "(VP dashed/VBN-HL (NP hope/NN-HL))\n",
      "(VP open/VB-HL (NP program/NN-HL))\n",
      "(VP Fleeting/VBG-HL (NP glimpse/NN-HL))\n",
      "(VP fragmented/VBN-HL (NP Society/NN-TL-HL))\n",
      "(VP locking/VBG-HL (NP bars/NNS-HL))\n",
      "(VP fire/VB (NP standard/JJ))\n",
      "(VP Canned/VBN-HL (NP cocktail/NN-HL frankfurters/NNS-HL))\n",
      "(VP whipped/VBN (NP Salt/NN Paprika/NN))\n",
      "(VP Barbecued/VBN-HL (NP frankfurters/NNS-HL))\n",
      "(VP Changing/VBG-HL (NP colors/NNS-HL))\n",
      "(VP Measuring/VBG-HL (NP armhole/NN-HL))\n",
      "(VP Backstitching/VBG-HL (NP seam/NN-HL))\n",
      "(VP Weaving/VBG-HL (NP seam/NN-HL))\n",
      "(VP drilling/VBG-HL (NP tools/NNS-HL))\n",
      "(VP drilling/VBG-HL (NP operations/NNS-HL))\n",
      "(VP Adjoining/VBG-HL (NP areas/NNS-HL))\n",
      "(VP chinning/VBG-HL (NP bar/NN-HL))\n",
      "(VP fattening/VBG-HL (NP rations/NNS-HL))\n",
      "(VP marketing/VBG-HL (NP methods/NNS-HL))\n",
      "(VP marketing/VBG-HL (NP management/NN-HL))\n",
      "(VP feeding/VBG-HL (NP facilities/NNS-HL))\n",
      "(VP Paid/VBN-HL (NP vacations/NNS-HL))\n",
      "(VP Eating/VBG-HL (NP facilities/NNS-HL))\n",
      "(VP farming/VBG-HL (NP methods/NNS-HL))\n",
      "(VP Nourishing/VBG (NP meals/NNS))\n",
      "(VP save/VB-HL (NP teeth/NNS-HL))\n",
      "(VP helps/VBZ-HL (NP families/NNS-HL))\n",
      "(VP Printed/VBN-HL (NP material/NN-HL))\n",
      "(VP Printed/VBN-HL (NP material/NN-HL))\n",
      "(VP Printed/VBN-HL (NP material/NN-HL))\n",
      "(VP Printed/VBN-HL (NP material/NN-HL))\n",
      "(VP planning/VBG-HL (NP process/NN-HL))\n",
      "(VP applying/VBG-HL (NP conditions/NNS-HL))\n",
      "(VP Encouraging/VBG-HL (NP self-help/NN-HL))\n",
      "(VP stressing/VBG-HL (NP self-help/NN-HL))\n",
      "(VP nearing/VBG-HL (NP self-sufficiency/NN-HL))\n",
      "(VP Advertising/VBG-HL (NP program/NN-HL))\n",
      "(VP Planning/VBG-HL (NP division/NN-HL))\n",
      "(VP financing/VBG-HL (NP adjustments/NNS-HL))\n",
      "(VP distributing/VBG-HL (NP funds/NNS-HL))\n",
      "(VP Matching/VBG-HL (NP requirements/NNS-HL))\n",
      "(VP prepared/VBN (NP shelter/NN))\n",
      "(VP Increased/VBN-HL (NP efficiency/NN-HL))\n",
      "(VP shifting/VBG-HL (NP styles/NNS-HL))\n",
      "(VP broadcasting/VBG-HL (NP station/NN-HL))\n",
      "(VP cleaning/VBG-HL (NP process/NN-HL))\n",
      "(VP frozen/VBN-HL (NP sections/NNS-HL))\n",
      "(VP Staining/VBG-HL (NP technique/NN-HL))\n",
      "(VP Concluding/VBG-HL (NP remarks/NNS-HL))\n",
      "(VP modernizing/VBG-HL (NP societies/NNS-HL))\n",
      "(VP teaching/VBG-HL (NP methods/NNS-HL))\n",
      "(VP teaching/VBG-HL (NP methods/NNS-HL))\n",
      "(VP bargaining/VBG-HL (NP issues/NNS-HL))\n",
      "(VP\n",
      "  selecting/VBG-HL\n",
      "  (NP mail/NN-HL questionnaire/NN-HL method/NN-HL))\n",
      "(VP mailing/VBG-HL (NP lists/NNS-HL))\n",
      "(VP distributed/VBN-HL (NP cost/NN-HL analysis/NN-HL))\n",
      "(VP define/VB-HL (NP input/output/NN-HL control/NN-HL system/NN-HL))\n",
      "(VP related/VBN-HL (NP materials/NNS-HL))\n",
      "(VP ionizing/VBG-HL (NP radiation/NN-HL))\n",
      "(VP seen/VBN (NP that/DT Af/NN))\n",
      "(VP\n",
      "  Chipping/VBG\n",
      "  (NP mechanism/NN)\n",
      "  (PP of/IN (NP cohesive/JJ failure/NN)))\n",
      "(VP cracking/VBG-HL (NP mechanism/NN-HL))\n",
      "(VP Processing/VBG-HL (NP urethanes/NNS-HL))\n",
      "(VP coupled/VBN-HL (NP image/NN-HL intensifiers/NNS-HL))\n",
      "(VP following/VBG (NP morning/NN))\n",
      "(VP whining/VBG (NP voice/NN))\n",
      "(VP convinced/VBN (PP of/IN (NP that/DT)))\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown = nltk.corpus.brown\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "for sent in brown.tagged_sents():\n",
    "    tree = cp.parse(sent)\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label()=='VP' :print(subtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. The bigram chunker scores about 90% accuracy. Using bigram_chunker.tagger.tag(postags) to examine the results and study its errors. Then experiment with trigram chunking. Are you able to improve the performance any more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t, c) for w, t, c in nltk.chunk.tree2conlltags(sent)]\n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.BigramTagger(train_data)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word, pos), chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t, c) for w, t, c in nltk.chunk.tree2conlltags(sent)]\n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.TrigramTagger(train_data)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word, pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word, pos), chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_chunker = BigramChunker(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.3%%\n",
      "    Precision:     82.3%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     84.5%%\n"
     ]
    }
   ],
   "source": [
    "print(bigram_chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_chunker = TrigramChunker(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.3%%\n",
      "    Precision:     82.5%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     84.6%%\n"
     ]
    }
   ],
   "source": [
    "print(trigram_chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Explore the Brown Corpus to print out all the FACILITIES (one of the commonly used types of name entities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(FACILITY Raymondville/NP)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY Kremlin/NP)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL House/NN-TL)\n",
      "(FACILITY White/JJ-TL House/NN-TL)\n",
      "(FACILITY Franklin/NP-TL)\n",
      "(FACILITY Kremlin/NP)\n",
      "(FACILITY Franklin/NP-TL Square/NN-TL)\n",
      "(FACILITY Pennsylvania/NP-TL Avenue/NN-TL)\n",
      "(FACILITY Jenks/NP-TL Street/NN-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL House/NN-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY Pensacola/NP)\n",
      "(FACILITY White/JJ-TL Sox/NPS-TL)\n",
      "(FACILITY Caltech/NP)\n",
      "(FACILITY White/JJ-TL House/NN-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY Caracas/NP)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL House/NN-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL Way/NN-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY Kremlin/NP)\n",
      "(FACILITY Kremlin/NP)\n",
      "(FACILITY Madison/NP-TL Square/NN-TL Garden/NN-TL)\n",
      "(FACILITY Boron/NP)\n",
      "(FACILITY Rome/NP)\n",
      "(FACILITY Rome/NP)\n",
      "(FACILITY Grafton/NP)\n",
      "(FACILITY Bari/NP)\n",
      "(FACILITY Bari/NP)\n",
      "(FACILITY Northfield/NP)\n",
      "(FACILITY Baltimore/NP)\n",
      "(FACILITY Hilo/NP)\n",
      "(FACILITY Hilo/NP)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL House/NN-TL)\n",
      "(FACILITY White/JJ-TL House/NN-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL House/NN-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL House/NN-TL)\n",
      "(FACILITY White/JJ-TL House/NN-TL)\n",
      "(FACILITY White/JJ-TL House/NN-TL)\n",
      "(FACILITY White/JJ-TL House/NN-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY White/JJ-TL)\n",
      "(FACILITY Kremlin/NP)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-f991cb928b20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbrown\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagged_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mne_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msubtree\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubtrees\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msubtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'FACILITY'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\chunk\\__init__.py\u001b[0m in \u001b[0;36mne_chunk\u001b[1;34m(tagged_tokens, binary)\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[0mchunker_pickle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_MULTICLASS_NE_CHUNKER\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[0mchunker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunker_pickle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mchunker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\chunk\\named_entity.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mEach\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtagged\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \"\"\"\n\u001b[1;32m--> 126\u001b[1;33m         \u001b[0mtagged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m         \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tagged_to_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\sequential.py\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[0mtags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\sequential.py\u001b[0m in \u001b[0;36mtag_one\u001b[1;34m(self, tokens, index, history)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtagger\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_taggers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m             \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\sequential.py\u001b[0m in \u001b[0;36mchoose_tag\u001b[1;34m(self, tokens, index, history)\u001b[0m\n\u001b[0;32m    646\u001b[0m         \u001b[1;31m# higher than that cutoff first; otherwise, return None.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cutoff_prob\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 648\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m         \u001b[0mpdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprob_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\classify\\maxent.py\u001b[0m in \u001b[0;36mclassify\u001b[1;34m(self, featureset)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprob_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprob_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\classify\\maxent.py\u001b[0m in \u001b[0;36mprob_classify\u001b[1;34m(self, featureset)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0mprob_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encoding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m             \u001b[0mfeature_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encoding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_logarithmic\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\classify\\maxent.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, featureset, label)\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m             \u001b[1;31m# Otherwise, we might want to fire an \"unseen-value feature\".\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unseen\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m                 \u001b[1;31m# Have we seen this fname/fval combination with any label?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mlabel2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_labels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for sent in brown.tagged_sents():\n",
    "    tree = nltk.ne_chunk(sent)\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'FACILITY': print(subtree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
